{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "## Feed Forward Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The activation function serves the purpose of forcing the successive layers of the neural network to be linearly independent (the successive weight matrices do not combine into one matrix).\n",
    "From a computational perspective, these operations are heavy with multiplications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The complexity of a square $\\left[n\\times n\\right]$ matrix multiplied by a $\\left[n\\times 1\\right]$ matrix is given by $O(n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Where a multiplication operation is assumed to have a complexity of $O(1)$. \n",
    "\n",
    "Each hidden layer has at least an matrix operation of this kind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The activation function's complexity is at least proportional to $O(\\log{(n)})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Hence, for every layer of the network $l$ (except the input layer), there is at least a matrix multiplication and at least an activation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If a given layer has 10 neurons, then the matrix operation has complexity of at least $O(100)$, while a layer of size 100 has complexity of at least $O(10\\,000)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Given that a typical CPU can run as many floating point operations in parallel as there are cores, a multi-cored system can reduce the effective run time as a factor of the number of cores. For instance, the 8$^{th}$ generation Intel i7 Processor has six cores. Six cores means a maximum of six multiplication operations for any given cycle which implies a reduced run-time by a factor of 6 at most. Use of all the cores would also leave the processor unavailable for other tasks. From this it can be seen that large ANNs can very quickly require large amounts of computational power; since any given forward pass of the NN requires each computation to be made again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "## FPGAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Configurable Logic Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "FPGAs allow for hardware level implementation of computational structures. FPGAs are generally made up of configurable logic blocks (CLBs); whose connections are configurable. The LE elements frequently consist of one or more look-up tables (LUTs), storage elements, carry logic, and multiplexers. These components allow the CLBs to simulate various logical operations from simple Boolean logic, to addition, to multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Congifurable Logic Block Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The logic elements are also networked  together by means of configurable buses in such away that logic elements across the length of the FPGA can be connected to carry out logical operations. These configurable elements allow for complex computational architectures to be implemented on the FPGA; from arithmetic logic units (ALUs) to digital signal processing (DSP) functions like digital filters and fast Fourier transforms (FFTs). Implementation of these architectures can serve any number of purposes, from rapid prototyping of computational architectures, to offloading necessary but repetitive computations from a microprocessor or CPU.\n",
    "\n",
    "The modularity of FPGAs CLBs, coupled with the configurable data bus system allows for repeated parallel structures. This lends itself well to implementing processors that are required to execute many similar operations in parallel rapidly, such as FFTs or digital filters. To that end FPGA manufacturers have been loading FPGAs with less configurable more dedicated hardware such as pre-optimised multiply-accumulators (MACs) known as DSP slices. DSP slices are in high-demand in these multiplication heavy computations. \n",
    "\n",
    "Given how well FPGAs seem to lend themselves to parallelism and modularity, it seems reasonable to assume that NNs could take advantage of this without the need for dedicated resources that could otherwise be used for other DSP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "## The binary spiking neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Synapse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Synapses are implemented as cyclic shift registers, where the output of the shift register feeds the input. The output of the shift register acts as an input to a binary two-input AND operator. The second input is the output of the presynaptic neuron, and the output of the AND operator is an input to a postsynaptic neuron. The function of the synapse is to regulate the firing rate of the presynaptic neuron, and the accumulation rate -and hence the firing rate- of the post-synaptic neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The guiding principle and intuition behind the synaptic implementation is that the synapse regulates the firing rate of the presynaptic neuron by modulating the probability of each spike reaching the post synaptic neuron. The result is that if the presynaptic neuron fires with frequency $f_{\\text{pre}}$ and the synapse is seeded with some probability $p_{\\text{synapse}}$ then the apparent firing rate at the postsynaptic neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "### Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Input Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The input neuron is fundamentally different from the remaining neurons in the network. The premise of the binary spiking neural network is that it reduces the complexity of the basic operations of a standard feed-forward neural network. The matrix multiplications of the standard FFNN are reduced to simultaneous binary AND operations across the network that execute in a single clock cycle. However, this construction requires a conversion of real world input parameters (typically some decimal value) into binary. The chosen method of conversion requires normalising the input value to some $p$ where"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
