{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Daniel Maidment---\n",
    "$\\renewcommand{\\vec}{\\mathbf}$\n",
    "$\\newcommand{\\x}{\\vec{x}}$\n",
    "$\\newcommand{\\X}{\\vec{X}}$\n",
    "$\\newcommand{\\s}{\\vec{s}}$\n",
    "$\\renewcommand{\\phi}{\\varphi}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\y}{\\vec{y}}$\n",
    "$\\renewcommand{\\v}{\\vec{v}}$\n",
    "$\\newcommand{\\A}{\\vec{A}}$\n",
    "$\\newcommand{\\I}{\\vec{I}}$\n",
    "$\\newcommand{\\z}{\\vec{z}}$\n",
    "$\\newcommand{\\a}{\\vec{a}}$\n",
    "$\\newcommand{\\b}{\\vec{b}}$\n",
    "$\\newcommand{\\t}{\\vec{t}}$\n",
    "$\\newcommand{\\W}{\\vec{W}}$\n",
    "$\\newcommand{\\w}{\\;\\!}$\n",
    "$\\newcommand{\\th}{\\text{th}}$\n",
    "$\\newcommand{\\(}{\\left (}$\n",
    "$\\newcommand{\\)}{\\right )}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed-forward neural networks (FFNN) are a ubiquitous classification technique whereby some input vector is passed through a series of linear transformations separated by an element wise activation functions. The output of the FFNN is ideally some non-linear function of the input that meaningfully classifies the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each linear transformation step can be thought of taking the inner product of some input vector with some weight matrix. The activation function is often a sigmoid function, or some approximation thereof. The activation function is constructed such that large input values saturate in the output, while small input values have an approximately linear output. The purpose of the activation function is to make each layer is a non-linear function of the previous layer. This is important because otherwise a feed-forward neural network can be represented as a single matrix operation, or a single fully connected layer. Another important and limiting factor of the activation function is that it should be differentiable so that the FFNN can be trained using backwards propagation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more holistic interpretation of the model would be to view each activation function as a neuron and the weight matrices as the synaptic connections between neurons. The sensing of the input vector results in the input neurons firing, these values are then propagated along the weighted synapses which are then combined as the input to the next neural layer. The activation function of neurons in the FFNN were initially modelled after organic neurons spike-response ***citation needed***, and to this day, many of th best activation functions are thought to be approximations of a neuronal spiking behaviour ***citation needed***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![]( images/Diagrams/feed_forward_neural_net.png)\\label{fig:ffnn}\n",
    "##### Simple fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational complexity and multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally multiplication of small decimals is a computationally taxing task. A typical measure of a processor's performance is the number of floating point operations per second (FLOPS). A floating point operations is effectively any multiplication, addition, subtraction, or division of two numbers represented in floating-point\\cite[brunvand_floating_2011]. These operations are typically carried out quickly, but on a single core sequentially. This is fine for everyday applications, however, in many signal processing tasks the number of FLOPs scales exponentially. This problem is well exemplified by neural networks, for instance, examine the neural network below. It is fully connected and hence, each set of weights can be described as a matrix of size $N\\times M$ where $N$ is the number of neurons in the post-synaptic layer, and $M$ is the number of neurons in the pre-synaptic layer. Each forward operation of the neural network is the dot product of the output of the pre-synaptic layer with the weight matrix, hence there are a minimum of $N M$ multiplication floating-point operations and $N(M-1)$ addition floating operations, or a total of $N(2M-1)$ floating point operations for a single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem of scale becomes clear upon closer examination. If the network is small and shallow as in fig.~\\ref{fig:ffnn} Then the number of floating point operations per layer operations (not counting activation functions) is 25. If a neuron is added to each layer the number becomes 56, make the network 10 layers deep, where each layer is 10 neurons (still considered a small neural network) and the figure becomes 1710. A common toy classification problem is optical character recognition of the MNIST dataset\\cite[lecun_mnist_nodate] where each image is $28x28$ pixels. Many classifiers work adequately with the MNIST dataset, but it is possible to use a FFNN, where there are 784 input neurons (1 for each pixel in an image from the dataset) and the output layer is 10 neurons, 1 for each decimal character. Assuming 1 hidden layer of 784 neurons the minimum number of floating point operations is 1244198. This is still considered a toy problem. Fundamentally the take away is that feed-forward neural networks are computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPGAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Field programmable gate-arrays\n",
    "* Dedicated multipliers\n",
    "* Resource scarcity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
